\subsection*{Micro-Behavior Results}

This section reviews the results of our predictive models of individual protest participation. The appendix includes table \ref{tab:predictiveopinionmodels} and demonstrates the coefficient estimates and some model fit statistics for our models of protest involvement. First, we assess the general predictive power of the models. Then we provide a brief discussion of select inter-group comparisons we discuss in the theoretical section of this chapter. Specifically, we focus on the coefficients for personal contact, network contact, and personal experiences with crime. In our earlier chapter, we discussed how minority populations within the host country may bear a disproportionate share of the burdens and costs associated with hosting a US military facility or large deployment. Accordingly, we also look at how minority respondents compare to non-minority respondents with respect to their likelihood of participating in anti-US protest activity.


%fix appendix references


We take multiple steps to assess the predictive power of the models. To provide a quick initial visual assessment we first present a series of separation plots in Figure \ref{fig:sepplotcombined}, each of which corresponds to one of the five models we discuss above. \citeasnoun{GreenhillWardSacks2011} present the separation plot as a means of visually assessing model performance. After estimating the models using the training data, we use the models to generate predicted probabilities for each observation in the test data. We then order the observations according to those predicted probabilities, with observations receiving the lowest predicted probabilities appearing on the left side of the plot and those observations with the highest predicted probabilities appearing on the right. Finally, we color coded each observation according to whether the binary response category for whether the individual participated in an anti-US protest is a yes (1) or a no (0).  Red indicates cases where the response was ``Yes'' and blue responses that were ``No''. The better the model performs the more red (``Yes'') observations we should see clustered on the right side of the panel, corresponding to higher predicted probability values. The five panels of Figure \ref{fig:sepplotcombined} correspond to the models shown in Appendix table \ref{tab:predictiveopinionmodels}. The ``Intercept Only'' model contains only the intercept values with no predictor variables. The model on the bottom is the full model, including the individual and group-level predictors, along with the varying coefficient estimates.

\begin{figure}[t]
	\centering\includegraphics[scale=0.7]{../../Figures/Chapter-Protests/figure-sep-plot-combined.pdf}
	\caption{Separation plot from various models of individual-level protest behavior. The black line shows the in-sample predicted probabilities of protest-involvement. We ordered the observations from low to high predicted values. The vertical red lines indicate observed ``Yes'' responses from the respondents. More red clustered towards the right side of the figure indicates stronger predictive performance.}
	\label{fig:sepplotcombined}
\end{figure}

From this figure we can see that the predictive performance of the models generally increases across the first three models. In the Intercept Only model, while there is obvious clustering towards the right side of the graph, there are also a large number of red observations scattered across the rest of graph space. The predicted values, as represented by the black line, are largely flat across the range of the graph, increasing only in a step-wise fashion at the far right end of the figure. We see some improvement once we move to the basic demographic model in the second panel, though there is still a considerable dispersion of ``Yes'' values across the figure. The model including both demographic and attitudinal/experiential variables produce some fairly significant gains. Here we see both clustering of red observations on the right side, and also more confidence in the predicted probability values towards the right side of the panel. Adding the group-level variables produces some additional gains, as does the inclusion of the varying coefficients in the final model. In both cases, we see some additional reduction in the number of ``Yes'' observations scattered across the panel, with a greater share of protest attendees clustered towards the right side.  

\input{../Tables/Chapter-Protests/model-protest-predict-stats.tex}

In general, we can see that these models become progressively better at correctly classifying observations in the test data. However, the separation plot is still a blunt instrument, and our ability to more precisely discriminate between the predictive performance of each model is limited. To supplement the information from the separation plots, Table \ref{tab:predictivecheck} contains additional statistics on the models' performance and predictive power. The first column shows the percent of cases that the model predicts correctly---that is, the model predicts protest involvement where a respondent reports protest involvement, and no protest involvement where a respondent does not report protest involvement.\footnote{Note that we use predicted probability values of 0.5 as the threshold for classifying predictions. Where $Pr(Protest | X_{ij}, Z_j) \geq 0.5 = 1$ or ``Yes'', and $Pr(Protest | X_{ij}, Z_j) < 0.5 = 0$ or ``No''. This simply means that where, conditional upon the values for our set of predictors for individual $i$ in country $j$, $X_{ij}$, and the country-level variables for country $j$, we observed a predicted probability of 0.5 or greater, we classify that individual as having a predicted value of ``Yes'', and ``No'' otherwise.} At first glance the models all appear to perform relatively well, with all models predicting at least 90\% of the cases correctly. However, given the overwhelming prevalence of ``No'' responses to the protest question in the data, this statistic alone can be misleading. Table \ref{tab:predictivecheck} contains several other statistics to help us understand their strengths and weaknesses. The false positive rate gives us an indication of how many individuals who replied ``No'' to the protest question were incorrectly classified by the model as having been involved in protest activity of some sort. In general, the models all have relatively low false positive rates, suggesting that we are not misclassifying too many ``No'' respondents. However, we do see some small increase in false positives as the models increase in complexity.

The false negative rate tells us how many ``Yes'' respondents the model misclassifies as not having been involved in protest activity. Here we start to get a better picture of the models' weaknesses and the sources of divergence in their performance. For example, the false negative rate of 82\% for the Intercept Only model means that the base model is incorrectly classifying 82\% of respondents who self-reported having been involved in anti-US protest. To rephrase slightly, if 100 individuals reported ``Yes'' when asked if they had previously attended an anti-US protest, the model would incorrectly predict 20 of these cases as ``No'' responses. We do see a reduction in the false negative rate as the models increase in complexity, but the lowest score we achieve is $\sim$51\%, which is still not very strong.

The sensitivity and specificity figures provide similar information, and while the names may be less than clear, their meanings are perhaps slightly more intuitive than the false negative and false positive metrics. Notably, these metrics are complements, summing to 100\% (i.e. false positive and specificity, and false negatives and sensitivity). Sensitivity tells us what percentage of ``Yes'' cases the model accurately predicts. Specificity tells us what percentage of ``No'' cases the model accurately predicts. A low sensitivity score means the model is doing a poor job of accurately predicting protest, whereas a high sensitivity score means that it is doing a better job of accurately predicting it. This statistic is appealing because it more directly speaks to the outcome of interest, protest, and the model's ability to recognize it when it occurs. Related, high specificity scores mean that the model is doing a better job of accurately predicting the \emph{non-events}---those cases where protest involvement did not happen. 

Predictably, the Intercept Only model does the poorest overall job of accurately classifying cases. The sensitivity score of 17\% indicates that only a small number of individuals who reported participating in anti-US protests are correctly classified as such. Alternatively, we can see that the specificity score of 99\% shows that the model did a good job of accurately classifying those individuals who reported never having attended an anti-US protest. However, such examples highlight the problems of relying solely on the percent correctly predicted metric---because the data are overwhelmingly ``No'' responses it is fairly easy to correctly classify those cases. But the model does a poor job of recognizing ``Yes'' responses when it sees them. If we think of the model as being akin to a genetic test---say one looking for some sort of chromosomal anomaly in a fetus---these results indicate that the test would correctly flag less than 20\% of such cases.

The models do better as we add more individual-level variables, and as we introduce more flexibility. That said, there are some diminishing returns. The largest increases in sensitivity come with the addition of the demographic variables, and then again when we add the attitudinal and experiential variables (i.e. models 2 and 3). In each case we see an increase of about 15 percentage points in the sensitivity score. This comes at the expense of specificity, which decreases slightly compared to the base model. Ultimately the most complex model only achieves a specificity score of approximately 49\%, meaning it is only correctly classifying about half of the instances where individuals have reported attending an anti-US protest event.

There are a few things to note here. To start, these scores are sensitive to the choice of cutpoints for classifying cases. In table \ref{tab:predictivecheck} we classify cases according to a fairly na\"{i}ve predicted probability threshold of 0.50. There are obviously an infinite number of alternative cutpoints that could be employed, but these entail trade-offs. Different predictive thresholds will affect sensitivity, specificity, and the overall share of accurate predictions. There are two ways that we can better understand the models' performance across these probability thresholds. First, Figure \ref{fig:protest-roc-plot} shows a receiver operating curve (ROC) plot for the five models we present. The ROC compares the true positive rate (specificity) and false positive rate ($1-$ specificity) for many different probability thresholds. Curves closer to the upper left corner are generally better at correctly classifying observations (such as pairing a ``Yes'' prediction with an individual who responded ``Yes'' to the question about attending anti-US protest events). The idea here is that there is an inherent tradeoff in classification---we can correctly classify all ``Yes'' cases if we use a very low probability threshold and classify every observation as a ``Yes'', but this necessarily means we end up with an enormous number of false positives (meaning that we classify observations as ``Yes'' when they are really a ``No''). Curves that are closer to the dashed reference line are equivalent to a random classification scheme.


\begin{figure}[t]
	\centering\includegraphics[scale=0.8]{../../Figures/Chapter-Protests/fig-roc-plot.png}
	\caption{Receiver operating curve (ROC) plot for the models shown in Table \ref{tab:predictiveopinionmodels}). This plot shows the percent of accurate ``Yes'' classifications relative to the percent of false positives (i.e. incorrect ``Yes'' classifications across a range of several different classification probability thresholds. Curves closer to the upper left represent better predictive accuracy.)}
	\label{fig:protest-roc-plot}
\end{figure}

From the curves presented in Figure \ref{fig:protest-roc-plot} we can see that there is indeed substantial variation in model performance when we account for the full range of probability thresholds. Ideally, we would want to see a model's curve spike up to 100\% immediately on the left side of the figure, but none of the models come close to this ideal type. The Intercept Only model performs the worst of the five, as we expect. Again, however, we see substantial improvements as we add demographic variables, and then demographic, attitudinal, and experiential variables. Ss this figure helps to make clearer, the relative performance of the three more complex models---including the one allowing for varying coefficient estimates---is nearly identical, with a very slight edge to the varying coefficient model. Ultimately, all of the models perform better than a simple random assignment mechanism, but some are clearly doing a better job of accurately classifying cases of protest involvement while sacrificing less in terms of generating a larger share of false positives.

\begin{figure}[t]
	\centering\includegraphics[scale=0.8]{../../Figures/Chapter-Protests/fig-pvalue-comparison.png}
	\caption{Comparison of sensitivity, specificity, and correct predictions across the full range of probability threshold values.)}
	\label{fig:protest-pvalue-compare}
\end{figure}

The disadvantage of the ROC plot is that it obscures the specific probability values and the trade-off between the different aspects of model performance that we seek to maximize. Figure \ref{fig:protest-pvalue-compare} plots the sensitivity, specificity, and percent predicted correctly figures across the range of possible classification probability thresholds. In general, higher probability thresholds make it more difficult for a given observation to make it into the ``Yes'' category. Conversely, lower thresholds mean that we are more likely to classify observations as ``Yes''. This is a seemingly trivial point but it is one worth reiterating because there is nothing special about the 0.50 threshold we use above---it is simply a convention of sorts, and one that is seemingly ``neutral''. But as we can see from Figure \ref{fig:protest-pvalue-compare} the trade-offs associated from moving between various probability thresholds are not all equal. In this case, our initial na\"{i}ve threshold of 0.50 sacrifices a considerable amount of performance on the sensitivity metric for relatively little gain on the specificity and overall correct prediction metrics. In fact, the percent of cases predicted correctly actually starts to decline slightly as we increase the probability threshold. To put it differently, setting too high a barrier for classification is also problematic as it means we miss an increasingly large share of cases.There are practical considerations in devising such a threshold as well. For example, if we found that the true probability of protesting were to be a 10\% chance, then we would be wrong nine of ten times. However, our model is providing information that these cases stand out from even lower-probability observations. Part of the decision calculus in choosing a threshold is judging the utility of having more false-positive or false-negatives and how that informs decision-making or policymaking. If a policymaker wants to guard against protests, then increasing the number of false positives to ensure that we capture the true positives is worthwhile. However, if the event itself is non-consequential or involves a severe policy response, then we should up our threshold and accept a higher number of false negatives. For example, if individuals who did not protest were subject to intensive surveillance or harassment because authorities \textit{believed} they were involved in protests, this sort of policy response could easily backfire and become a self-fulfilling prophecy that makes protest behavior \textit{more likely}. Returning to the table, across all five models we can see that lower classification thresholds generate relatively comparable figures for specificity and overall correct predictions, while yielding much better performance on the sensitivity metric. Referring to Figure \ref{fig:sepplotcombined} helps to clarify this point, as the vast majority of cases have very low predicted probability values assigned to them. Using a lower probability threshold for determining which observations fall into the ``Yes'' category can thus improve overall predictive accuracy.


\input{../Tables/Chapter-Protests/model-protest-predict-stats-targeted.tex}

Table \ref{tab:predictivechecktargeted} shows the models' predictive accuracy when we use 0.10 as the probability threshold for predicting individuals' protest experience. In general, the overall percent of observations correctly predicted remains fairly high but has declined slightly. We also see the false positive rate has increased, which we should expect from having a much lower threshold. More importantly, the sensitivity score has increased substantially across each model. The minimum here is approximately 41\%, with a high of 77\%. In other words, using a lower probability threshold our final model is correctly classifying three-quarters of cases where individuals report having participated in anti-US protest events. Again, this increase does come with a cost. The lower overall rate of correct predictions results from the slight decrease in the correct predictions of ``No'' responses. The increased false positive rate also reflects this. 

As one last check on our models' performance, Figure \ref{fig:ppcheck-individual-protest} shows the results of a set of posterior predictive checks using the five protest models discussed in this section. For each model, we generate 1,000 simulated data sets, each containing simulated predicted Yes/No values for the outcome variable. For each data, set we take the mean value of these predictions, giving us the proportion of each simulated data set where the model predicts individuals to have responded ``Yes'' to the outcome variable question. The light blue histograms in each panel shows the distribution of these predicted proportion values. The dark line shows the actual proportion of ``Yes'' responses in the real survey data. The narrow bands on the X-axis somewhat mask the fact that the actual distribution of simulated values is fairly narrow in every case. All five models produce reasonably close approximations of the real data, though the mean values are slightly below the actual mean value in every panel except the Intercept Only model. The model including only respondents' demographic characteristics actually produces the closest set of simulated values, though with slightly greater dispersion than the other more complex and fully specified models. 

Ultimately the models all produce fairly accurate aggregate-level predictions. Regarding individual-level classifications. The appropriate balance for selecting probability thresholds is somewhat subjective and depends upon the specific goals one has in mind, but these comparisons help to show that our models are reasonably flexible and accurate when it comes to predicting individuals' likelihood of participating in protests against the United States. 



\begin{figure}[t]
	\centering\includegraphics[scale=0.8]{../../Figures/Chapter-Protests/fig-opinion-ppcheck-plots.png}
	\caption{Posterior predictive check on individual-level protest models. The black bar represents the proportion of ``Yes'' responses to the protest attendance question in the test data (i.e. the number of people who say they've been to an anti-US protest event. The light blue area represents the distribution of the estimated proportion of protest attendees based on 1,000 simulations from the training data.).}
	\label{fig:ppcheck-individual-protest}
\end{figure}


In addition to the predictive accuracy of the models we are also interested in assessing comparisons between groups. The individual coefficient estimates for a few key variables can help us to better understand these patterns. Figure \ref{fig:coefplot-population-compare} shows the coefficients for the minority status, personal contact, network contact, and experience with crime variables from four of the models (except the Intercept Only model). The estimates are all positive, and all fairly consistent across models. The coefficient on the variable capturing reported experiences with crime produce the most variation of the four. Individuals who have experienced a crime involving US military personnel are far more likely to be involved in anti-US protest events than individuals who lack such experiences. The coefficient values here are approximately 2, indicating that individuals who are the victim of a crime perpetrated by a US service member are approximately 50\% more likely to participate in anti-US protest activity than people who responded ``No'' to this question. Contrast this with the minority and contact coefficients---0.85 and 0.62, respectively---indicating a roughly 20\% and 15\% higher probability of participating in anti-US protests than non-minority respondents, and those who report not having contact with US service personnel. Individuals who have experienced a crime involving a US service member are much more likely to mobilize politically against the United States military than those who have not. 


\begin{figure}[t]
	\centering\includegraphics[scale=0.65]{../../Figures/Chapter-Protests/fig-coefplot-population-opinion-model-compare.png}
	\caption{Coefficient estimates for models 1, 2, 3, and 4 presented above. These coefficient estimates exclude those for the varying coefficients model, which we include in a separate figure. 50\%, 80\% and 95\% credible intervals shown around median highest density probability prediction.}
	\label{fig:coefplot-population-compare}
\end{figure}

Finally, Figure \ref{fig:coefplot-varying-coefficients} shows the coefficient estimates from the varying coefficient model broken down by country. Though the general patterns look similar to those we see for the population-level estimates, there are some notable differences. First, as with the population-level coefficients, we again see that individuals reporting having experienced a crime involving US service personnel are far more likely to report protest participation than those who have not had these experiences. While this difference is by far the largest across all 14 countries, there is some noteworthy variation in just how big these comparisons are. The differences between groups tend to be smallest in Kuwait, which stands to reason given its authoritarian government and the difficulty of protesting under such conditions.  The median estimate for Japan is the largest of all of the countries, but the dispersion around the point prediction is quite larger. This results from only having 21 respondents out of $\sim$3,000 total respondents in Japan reported having experienced such a crime. This is not the case in every country, however. In Italy, 90 individuals reported such experiences with crime. In Portugal, 116 individuals reported experiencing a crime involving US service personnel. In Kuwait, 837. We provide a fuller review of the relationship between US military deployments and crime in the previous chapter, but we should reiterate two points here: although these cases are a clear minority in our data, we have a sufficient number of observations to generate reasonable coefficient estimates. The relatively small number of observations does lead to an increase in the error around the predicted estimates. Even allowing for this larger error, though, the entirely range of these coefficient estimates tends to be quite a bit larger than the other coefficient estimates. 




\begin{figure}[t]
	\centering\includegraphics[scale=0.65]{../../Figures/Chapter-Protests/fig-coefplot-varyingeffect-opinion-model-5.png}
	\caption{Country level coefficient estimates for the varying coefficient models of individual-level protest behavior. 50\%, 80\% and 95\% credible intervals shown around median highest density probability prediction.}
	\label{fig:coefplot-varying-coefficients}
\end{figure}

Second, it is entirely possible that there is under-reporting on questions such as this one. We cannot currently say how this under-reporting may affect these inter-group comparisons. It is entirely possible that individuals who choose to not report experiencing such crimes may also not wish to engage in protest activity against the US Survivors of sexual assault, or other violent crimes, may struggle with trauma in the aftermath of their experiences, seeking to avoid situations or activities that require them to interact with US service personnel or expose themselves to harm. Given that the risk of violent confrontation with authorities, or even co-nationals, is higher at protest events, individuals may choose to not participate. In such cases, we might expect the treatment effect of crime to vary based on other conditioning factors. For example, as we note in this paragraph, the type of crime may matter in determining how, and if, individuals choose to mobilize politically against the US military.








